# Project Guess the Book Scripts
Scripts from Guess the Book project - training an AI to guess author names based on symbol frequencies of their works.

## Some ready-made datasets in this repo
> Based on Project Gutenberg's library of public domain works downloaded on 2019/07/03.

I've included the following datasets to play with:

### 01_Raw Metadata_+_Symbol_Frequency_Set.csv
The initial merged dataset from 04_merge.py. Columns are:
- Author (author full names)
- Birthdate (year of author's birth - may not be consistent for a given author)
- Deathdate (year of author's death  - may not be consistent for a given author)
- Work_Title (name of the book)
- 243 columns of symbol frequencies - dense in the middle, sparse on either side.

### X_mi/trees_Y_features.csv
Datasets generated by 06_mic_trees_feature_selection.py. X stands for the number of books per author in a given set, "mi" or "trees" stands for the method of feature selection (mi: mutual info classifier, trees: classification trees), Y - number of features to train on.


## Where to get all the (fresh) data
The scripts work based on Project Gutenberg's library of public domain works. You can download the metadata about the books here: http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs
And the instructions about how to get the actual books here: http://www.gutenberg.org/wiki/Gutenberg:Mirroring_How-To (mirroring the site),
and here: http://www.gutenberg.org/wiki/Gutenberg:Information_About_Robot_Access_to_our_Pages (downloading a specific subset of books).


## Included scripts:

### 01_rdf_to_csv.py: 
Takes a path where .rdf/xml files are stored, extracts author names, years of birth & death, and titles of their works, puts them into a Pandas dataframe, and saves them as a .csv file in the location of your choice.

### 02_book-metadata_analysis:
Takes in the .csv generated by 01_rdf_to_csv.py, and runs basic analysis on author names and numbers, their birth/death years, and lifespan (+ some sanity checking).

### 03_frequentize.py:
Takes a path where .txt files are stored, returns a .csv with symbol frequency table, one row per .txt file.

### 04_merge.py:
Merges the data from 01_rdf_to_csv.py and 03_frequentize.py into a single table, cleans empty entries.

### 05_scaling_splitting_sets.py:
Scales the dataset from 4_merge.py to [0, 1], and splits it into training sets based on the number of books per author. Fills in empty values with 0s, and removes entries with empty frequency rows.

### 06_mic_trees_feature_selection.py:
Takes in the split sets from 05_scaling_splitting_sets.py, identifies top features based on mutual info classifier and classification trees, and saves the new datasets with those features and author names only.

### 07_AI_tests.py:
Takes the datasets from 06_mic_trees_feature_selection.py, runs them through 4 AIs (neural net, classification trees, SVMs, Gaussian Bayes) with 10-fold cross-validation, reports on the accuracy scores per AI per dataset, and saves the report as a .csv file.
